This is where it gets exciting. Let me break down the full landscape of ML/AI options for building a proprietary receipt intelligence layer, from pragmatic short-term approaches to sophisticated long-term plays.

---

## The Training Data Foundation

Before we talk models, let's talk data. **Your model is only as good as your data.**

### What Data to Collect (Starting NOW)

```typescript
// Enhanced transaction schema for ML training
interface TrainingDataPoint {
  // Original inputs
  receiptImageBase64: string;
  receiptImageUrl: string;  // Store in Cloud Storage
  imageMetadata: {
    width: number;
    height: number;
    format: 'jpeg' | 'png' | 'webp';
    captureDevice?: string;
    lightingQuality?: 'good' | 'medium' | 'poor';
  };
  
  // AI extraction (what Gemini returned)
  aiExtraction: {
    merchant: string;
    date: string;
    total: number;
    items: Array<{name: string; price: number; category?: string}>;
    confidence?: number;
    rawJsonResponse: string;
  };
  
  // Human corrections (GOLD for training)
  humanCorrections: {
    merchantCorrected?: string;
    dateCorrected?: string;
    totalCorrected?: number;
    itemsCorrected?: Array<{name: string; price: number; category?: string}>;
    categoryCorrected?: string;
    correctionTimestamp: Date;
  };
  
  // Metadata
  region: 'CL' | 'CO' | 'MX' | 'AR';
  storeType: 'supermarket' | 'pharmacy' | 'restaurant' | 'gas_station' | 'other';
  userId: string;  // For consent tracking
  consentForTraining: boolean;
  createdAt: Date;
}
```

### Data Volume Targets

| Milestone | Receipts | What You Can Do |
|-----------|----------|-----------------|
| **1,000** | Basic patterns | Evaluate Gemini accuracy, identify problem areas |
| **10,000** | Fine-tuning viable | Train adapters, build merchant normalization |
| **50,000** | Competitive advantage | Fine-tune vision models, regional specialization |
| **100,000+** | Serious moat | Train custom models, sell B2B intelligence |

---

## Model Architecture Options

### Tier 1: Enhanced Prompting (Now - 3 months)

**No training required, immediate improvement**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CURRENT ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Receipt Image â†’ Gemini API â†’ JSON Response â†’ App Display   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENHANCED ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Receipt Image                                               â”‚
â”‚       â†“                                                      â”‚
â”‚  Pre-processing (rotation, contrast, crop)                   â”‚
â”‚       â†“                                                      â”‚
â”‚  Gemini API + Dynamic Few-Shot Examples                      â”‚
â”‚       â†“                                                      â”‚
â”‚  Post-processing (merchant normalization, validation)        â”‚
â”‚       â†“                                                      â”‚
â”‚  Confidence Scoring                                          â”‚
â”‚       â†“                                                      â”‚
â”‚  App Display (with "verify" prompt if low confidence)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key improvements:**
- **Few-shot examples**: Include 3-5 Chilean receipt examples in your prompt
- **Merchant lookup**: Post-process against your merchant database
- **Confidence thresholds**: If uncertain, ask user to verify

**Cost:** ~$0 additional (same Gemini API usage)

---

### Tier 2: Fine-Tuned LLM Adapters (3-6 months)

**When you have 5,000+ receipts with corrections**

#### Option A: Gemini Fine-Tuning (Google Cloud)

```python
# Gemini fine-tuning via Vertex AI
from google.cloud import aiplatform

# Prepare training data in JSONL format
training_data = [
    {
        "input": "<receipt_image_base64>",
        "output": {
            "merchant": "Jumbo Las Condes",
            "date": "2024-12-15",
            "total": 45670,
            "items": [...]
        }
    },
    # ... thousands more examples
]

# Fine-tune Gemini
tuned_model = aiplatform.Model.upload(
    display_name="gastify-receipt-parser-cl",
    base_model="gemini-1.5-flash",
    training_data=training_data,
    # ... configuration
)
```

**Pros:** 
- Stays in Google ecosystem
- Easy integration with existing code
- Good multimodal support

**Cons:**
- Vendor lock-in
- Less control over training process
- Cost scales with usage

**Estimated cost:** $500-2,000 for fine-tuning + ongoing API costs

---

#### Option B: Open Source LLM Fine-Tuning

**Models to consider:**

| Model | Size | Multimodal | Best For |
|-------|------|------------|----------|
| **LLaVA 1.6** | 7B-34B | âœ… Yes | Best open multimodal option |
| **Qwen2-VL** | 7B-72B | âœ… Yes | Strong OCR capabilities |
| **PaliGemma** | 3B | âœ… Yes | Google's open vision-language model |
| **Mistral 7B** | 7B | âŒ No | Text processing after OCR |
| **Phi-3 Vision** | 4.2B | âœ… Yes | Lightweight, good for edge |

**Recommended: Qwen2-VL-7B** for receipt parsing

```python
# Fine-tuning Qwen2-VL with LoRA adapters
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import LoraConfig, get_peft_model

# Load base model
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Add LoRA adapters (efficient fine-tuning)
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# Train on your Chilean receipt dataset
trainer = Trainer(
    model=model,
    train_dataset=chilean_receipts_dataset,
    # ... training args
)
trainer.train()
```

**Why LoRA?**
- Only trains ~1% of parameters
- Can train on a single A100 GPU
- Keeps base model knowledge intact
- Easy to swap adapters (Chile vs Colombia vs MÃ©xico)

---

### Tier 3: Specialized OCR + Classification Pipeline (6-12 months)

**When you need maximum accuracy and control**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPECIALIZED PIPELINE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Receipt    â”‚    â”‚  Document    â”‚    â”‚    Text      â”‚           â”‚
â”‚  â”‚    Image     â”‚ â†’ â”‚  Detection   â”‚ â†’ â”‚   Regions    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (YOLO/DETR) â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                   â”‚
â”‚                                                   â†“                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Structured  â”‚    â”‚   Field      â”‚    â”‚    OCR       â”‚           â”‚
â”‚  â”‚    JSON      â”‚ â† â”‚  Extraction  â”‚ â† â”‚  (PaddleOCR) â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   (LayoutLM) â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚         â†“                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Final      â”‚    â”‚  Category    â”‚    â”‚  Merchant    â”‚           â”‚
â”‚  â”‚   Output     â”‚ â† â”‚  Classifier  â”‚ â† â”‚  Normalizer  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (BERT-tiny) â”‚    â”‚  (fuzzy+DB)  â”‚           â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Component Models:**

| Component | Model | Size | Purpose |
|-----------|-------|------|---------|
| **Document Detection** | YOLOv8 or RT-DETR | 20-50MB | Find receipt in image |
| **OCR** | PaddleOCR or EasyOCR | 100MB | Extract text from image |
| **Layout Understanding** | LayoutLMv3 | 400MB | Understand receipt structure |
| **Field Extraction** | Fine-tuned DistilBERT | 250MB | Extract merchant, date, total |
| **Category Classification** | BERT-tiny or TF-IDF + LogReg | 20MB | Classify spending category |
| **Merchant Normalization** | RapidFuzz + PostgreSQL | N/A | Normalize merchant names |

**Total pipeline size:** ~800MB (can run on CPU)

---

## Infrastructure Options

### Option 1: Serverless (Recommended for Start)

**Google Cloud Run + Vertex AI**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVERLESS ARCHITECTURE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   React App                                                  â”‚
â”‚       â”‚                                                      â”‚
â”‚       â†“                                                      â”‚
â”‚   Cloud Functions (Node.js)                                  â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”œâ”€â†’ Vertex AI (Gemini fine-tuned) â”€â”€â†’ JSON response   â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”œâ”€â†’ Cloud Storage (receipt images)                     â”‚
â”‚       â”‚                                                      â”‚
â”‚       â””â”€â†’ Firestore (transactions + training data)          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Estimated costs (1,000 users, 5,000 scans/month):**
- Vertex AI: $50-100/month
- Cloud Functions: $10-20/month
- Cloud Storage: $5-10/month
- **Total: ~$70-130/month**

---

### Option 2: Hybrid (Medium Scale)

**Cloud Run for API + Modal/Replicate for ML**

```python
# Using Modal for GPU inference (pay-per-use)
import modal

app = modal.App("gastify-ml")

@app.function(gpu="A10G", image=modal.Image.debian_slim().pip_install("transformers", "torch"))
def analyze_receipt(image_base64: str) -> dict:
    # Load fine-tuned model
    model = load_model("gastify-qwen2vl-chile")
    
    # Process receipt
    result = model.generate(image_base64)
    
    return result
```

**Why Modal/Replicate?**
- Pay only when running inference
- No GPU management
- Auto-scaling
- ~$0.001-0.01 per receipt

**Estimated costs (5,000 scans/month):**
- Modal GPU: $25-50/month
- Cloud Run API: $20/month
- **Total: ~$50-70/month**

---

### Option 3: Self-Hosted (Scale/Control)

**Kubernetes + GPU Nodes**

```yaml
# Kubernetes deployment for ML service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gastify-ml-service
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: receipt-parser
        image: gcr.io/gastify/receipt-parser:v1
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            memory: "8Gi"
            cpu: "2"
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-tesla-t4
```

**When to consider:**
- 50,000+ scans/month
- Need full control over models
- B2B offering planned

**Estimated costs:**
- GKE with T4 GPU: $300-500/month
- But: Better unit economics at scale

---

## Regional Model Strategy

**Key insight:** Train separate adapters for each country

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-REGION MODEL ARCHITECTURE           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Base Model (Qwen2-VL-7B or Gemini)                        â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”œâ”€â†’ LoRA Adapter: Chile ğŸ‡¨ğŸ‡±                            â”‚
â”‚       â”‚   - Jumbo, LÃ­der, Santa Isabel patterns             â”‚
â”‚       â”‚   - RUT format, boleta electrÃ³nica                  â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”œâ”€â†’ LoRA Adapter: Colombia ğŸ‡¨ğŸ‡´                         â”‚
â”‚       â”‚   - Ã‰xito, Carulla, D1 patterns                     â”‚
â”‚       â”‚   - NIT format, factura electrÃ³nica                 â”‚
â”‚       â”‚                                                      â”‚
â”‚       â””â”€â†’ LoRA Adapter: MÃ©xico ğŸ‡²ğŸ‡½                           â”‚
â”‚           - Walmart, Soriana, OXXO patterns                 â”‚
â”‚           - RFC format, CFDI requirements                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- Each region gets specialized accuracy
- Swap adapters based on user location
- Faster training (only adapter weights)
- A/B test improvements per region

---

## Implementation Roadmap

### Phase 1: Data Collection (Now - Month 2)
```
Week 1-2:  Add image storage to Cloud Storage
Week 3-4:  Build correction tracking in Firestore
Week 5-6:  Add consent flow for training data
Week 7-8:  Create data export pipeline
```

### Phase 2: Enhanced Prompting (Month 2-3)
```
Week 1-2:  Build Chilean receipt few-shot examples
Week 3-4:  Implement merchant normalization database
Week 5-6:  Add confidence scoring
Week 7-8:  A/B test against baseline
```

### Phase 3: Fine-Tuning (Month 4-6)
```
Week 1-4:   Prepare training dataset (10K+ receipts)
Week 5-8:   Fine-tune Qwen2-VL with LoRA
Week 9-10:  Deploy to Modal/Replicate
Week 11-12: Shadow mode (run both, compare)
```

### Phase 4: Specialized Pipeline (Month 7-12)
```
Month 7-8:  Build OCR + LayoutLM pipeline
Month 9-10: Train category classifier
Month 11-12: Full pipeline deployment
```

---

## Quick Wins You Can Implement This Week

### 1. Start Storing Images

```typescript
// services/storage.ts
import { getStorage, ref, uploadBytes, getDownloadURL } from 'firebase/storage';

export async function storeReceiptImage(
  userId: string,
  imageBase64: string,
  transactionId: string
): Promise<string> {
  const storage = getStorage();
  const imageRef = ref(storage, `receipts/${userId}/${transactionId}.jpg`);
  
  // Convert base64 to blob
  const response = await fetch(imageBase64);
  const blob = await response.blob();
  
  await uploadBytes(imageRef, blob);
  return getDownloadURL(imageRef);
}
```

### 2. Track Corrections

```typescript
// services/corrections.ts
export interface CorrectionEvent {
  transactionId: string;
  field: 'merchant' | 'date' | 'total' | 'category' | 'items';
  originalValue: any;
  correctedValue: any;
  timestamp: Date;
}

export async function logCorrection(
  user: User,
  correction: CorrectionEvent
): Promise<void> {
  const correctionsRef = collection(
    db, 
    `artifacts/${appId}/users/${user.uid}/corrections`
  );
  await addDoc(correctionsRef, correction);
}
```

### 3. Build Merchant Database

```typescript
// utils/merchants.ts
export const CHILEAN_MERCHANTS: Record<string, string> = {
  // Supermarkets
  'JUMBO': 'Jumbo',
  'LIDER': 'LÃ­der',
  'LIDER EXPRESS': 'LÃ­der Express',
  'SANTA ISABEL': 'Santa Isabel',
  'UNIMARC': 'Unimarc',
  'TOTTUS': 'Tottus',
  
  // Pharmacies
  'CRUZ VERDE': 'Cruz Verde',
  'SALCOBRAND': 'Salcobrand',
  'AHUMADA': 'Farmacias Ahumada',
  
  // ... 500+ more
};

export function normalizeMerchant(rawMerchant: string): string {
  const upper = rawMerchant.toUpperCase().trim();
  
  // Exact match
  if (CHILEAN_MERCHANTS[upper]) {
    return CHILEAN_MERCHANTS[upper];
  }
  
  // Fuzzy match
  const fuzzyMatch = findBestMatch(upper, Object.keys(CHILEAN_MERCHANTS));
  if (fuzzyMatch.rating > 0.8) {
    return CHILEAN_MERCHANTS[fuzzyMatch.target];
  }
  
  return rawMerchant; // Return original if no match
}
```

---

## Cost Comparison Summary

| Approach | Monthly Cost | Accuracy | Effort | When |
|----------|--------------|----------|--------|------|
| **Gemini + Better Prompts** | $20-50 | 85% | Low | Now |
| **Gemini Fine-tuned** | $100-200 | 92% | Medium | 3-6 months |
| **Open Source (Modal)** | $50-100 | 90-95% | High | 6+ months |
| **Full Pipeline (Self-hosted)** | $300-500 | 95%+ | Very High | 12+ months |

---

## My Recommendation

**For Gastify's current stage:**

1. **Immediately:** Start storing images + corrections (this is your future gold mine)
2. **Month 1-2:** Build merchant normalization database for Chile
3. **Month 3-4:** Experiment with Gemini fine-tuning via Vertex AI
4. **Month 6+:** Evaluate open-source options when you have 10K+ receipts

The key is **not to over-engineer now**, but **collect the data that will make future training possible**. Every receipt you process today without storing the image and corrections is lost training data.

Â¿Quieres que profundicemos en algÃºn aspecto especÃ­fico? Puedo ayudarte a:
- DiseÃ±ar el schema de Firestore para corrections
- Escribir el cÃ³digo de storage de imÃ¡genes
- Crear el MVP del merchant normalization
- Evaluar costos especÃ­ficos de Vertex AI